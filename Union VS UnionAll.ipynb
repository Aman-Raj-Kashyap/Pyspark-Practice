{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11119e92-4db9-42e4-aef2-ae5d5ead020e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "union(): used to merge two dataframe's of the same structure/schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ed00988-c9da-4f9a-9440-7a6484631f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "unionAll(): deprecated since pyspark \"2.0.0\" version and recommends using the union() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933cc352-d279-4984-a72a-22cf6442fa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ff6349-2605-41eb-adfd-8d831ba52535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+\n|employee_name|employee_gender|employee_salary|\n+-------------+---------------+---------------+\n|        Alice|              F|          70000|\n|          Bob|              M|          80000|\n|      Charlie|              M|          55000|\n|        David|              M|          45000|\n|          Eve|              F|          50000|\n|          Eve|              F|          50000|\n+-------------+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "data_1 = [Row(employee_name=\"Alice\", employee_gender=\"F\", employee_salary=70000),\n",
    "         Row(employee_name=\"Bob\", employee_gender=\"M\", employee_salary=80000),\n",
    "         Row(employee_name=\"Charlie\", employee_gender=\"M\", employee_salary=55000),\n",
    "         Row(employee_name=\"David\", employee_gender=\"M\", employee_salary=45000),\n",
    "         Row(employee_name=\"Eve\", employee_gender=\"F\", employee_salary=50000),\n",
    "          Row(employee_name=\"Eve\", employee_gender=\"F\", employee_salary=50000)\n",
    "         ]\n",
    "        \n",
    "df_1 = spark.createDataFrame(data_1)\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e65e75f-dc73-4fed-9ea8-dcb47ae4139c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+\n|employee_name|employee_gender|employee_salary|\n+-------------+---------------+---------------+\n|        Frank|              M|          60000|\n|        Grace|              F|          65000|\n|       Hannah|              F|          70000|\n|          Ian|              M|          48000|\n|         Jill|              F|          53000|\n|          Eve|              F|          50000|\n+-------------+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "data_2 = [Row(employee_name=\"Frank\", employee_gender=\"M\", employee_salary=60000),\n",
    "         Row(employee_name=\"Grace\", employee_gender=\"F\", employee_salary=65000),\n",
    "         Row(employee_name=\"Hannah\", employee_gender=\"F\", employee_salary=70000),\n",
    "         Row(employee_name=\"Ian\", employee_gender=\"M\", employee_salary=48000),\n",
    "         Row(employee_name=\"Jill\", employee_gender=\"F\", employee_salary=53000),\n",
    "          Row(employee_name=\"Eve\", employee_gender=\"F\", employee_salary=50000)\n",
    "        ]\n",
    "\n",
    "df_2 = spark.createDataFrame(data_2)\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74fd0eb5-0d3b-4b66-9650-9c2aa5a03b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+\n|employee_name|employee_gender|employee_salary|\n+-------------+---------------+---------------+\n|        Alice|              F|          70000|\n|          Bob|              M|          80000|\n|      Charlie|              M|          55000|\n|        David|              M|          45000|\n|          Eve|              F|          50000|\n|        Frank|              M|          60000|\n|        Grace|              F|          65000|\n|       Hannah|              F|          70000|\n|          Ian|              M|          48000|\n|         Jill|              F|          53000|\n+-------------+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_1_union_df_2 = df_1.union(df_2).distinct()\n",
    "\n",
    "#union don't remove duplicate value, it only remove in sql\n",
    "\n",
    "df_1_union_df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb2cb37-f8cc-4a42-8d1a-b6885de91ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dealing when schemas are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85f5539-1e39-4116-a76e-f7120038dd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n|col_0|col_1|col_2|\n+-----+-----+-----+\n|   10|   20|   30|\n|  100|  200|  300|\n+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Dataframes with same column names\n",
    "\n",
    "data_1 = [[10,20,30]]\n",
    "schema_1 = \"col_0 int , col_1 int , col_2 int\"\n",
    "df_1 = spark.createDataFrame(data = data_1, schema = schema_1)\n",
    "\n",
    "data_2 = [[100,200,300]]\n",
    "schema_2 = \"col_0 int , col_1 int , col_2 int\"\n",
    "df_2 = spark.createDataFrame(data = data_2, schema = schema_2)\n",
    "\n",
    "#performing union\n",
    "df_3 = df_1.unionByName(df_2)\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13582a68-edde-4e8f-b91c-b0d98d9e940f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n|col_0|col_1|col_2|col_3|\n+-----+-----+-----+-----+\n|   10|   20|   30| null|\n| null|  100|  200|  300|\n+-----+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Dataframes with different column names\n",
    "\n",
    "data_1 = [[10,20,30]]\n",
    "schema_1 = \"col_0 int , col_1 int , col_2 int\"\n",
    "df_1 = spark.createDataFrame(data = data_1, schema = schema_1)\n",
    "\n",
    "data_2 = [[100,200,300]]\n",
    "schema_2 = \"col_1 int , col_2 int , col_3 int\"\n",
    "df_2 = spark.createDataFrame(data = data_2, schema = schema_2)\n",
    "\n",
    "#performing union on df1 & df2\n",
    "# df_3 = df_1.unionByName(df_2) --giving error\n",
    "\n",
    "df_3 = df_1.unionByName(df_2, allowMissingColumns = True)\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ffabf2c-8760-442f-9930-b9d1dea0fc36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Union VS UnionAll",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
